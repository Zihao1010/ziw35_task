Task3From the finding in the task2, we find that the scores in the beamSearchV2 are much larger than that in the beamSearchV1. In the v2, we add the length normalization to achieve this algorithm. At first, I change the value of lambda to 0.5, I find the score of the ¡°it is¡± decrease and the score of other two sentence increase. And when I change the value to less than 0.1, the score of there sentences will almost be constant. And when I change the value of lambda, the content of first sentence and third sentence will be changed because of the length of sentence. We can see that the length of sentence and the value of lambda will change the result. We add a additional parameter to restrict the result instead of only depending on the value of product of probability.Here the normalization coefficient is the length of the sentence, that is, divided by the number of words in the sentence. You can also add an index, which can also be treated as a hyperparameter. By dividing by the number of words in the translation result. This is the average of the logarithm of the probability of each word, which obviously reduces the penalty for outputting long results. But the limitation is that this algorithm is not the best choice so it will get a bigger error.